{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym Warm-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space of the Env:  16\n",
      "State space of the Env by accessing env.nS:  16\n",
      "Action space of the Env:  4\n"
     ]
    }
   ],
   "source": [
    "# Import Environment class and Libraries\n",
    "from frozen_lake import FrozenLakeEnv\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Create Environment Object\n",
    "env = FrozenLakeEnv(map_name =\"4x4\", is_slippery=False)\n",
    "\n",
    "\n",
    "# Access the number of states:\n",
    "nS = env.observation_space\n",
    "print(\"State space of the Env: \", nS)\n",
    "\n",
    "# or you could even use \n",
    "nS = env.nS\n",
    "print(\"State space of the Env by accessing env.nS: \", nS)\n",
    "\n",
    "\n",
    "# Action space of the agent:\n",
    "nA = env.nA\n",
    "print(\"Action space of the Env: \", nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probabilty:  1.0\n",
      "Next State:  8\n",
      "Reward:  0.0\n",
      "Episode ended? :  False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For policy iteration, you would need to access\n",
    "State(s), Action(a), Next State(ns), Reward(r), episode ended? (is_done) tuples.\n",
    "\n",
    "Note that in this environment, the orientation of the agent does not matter.\n",
    "No matter what direction the agent is facing, if, say a left action is performed, \n",
    "the agent moves to the left of the crrent state.\n",
    "\"\"\"\n",
    "\n",
    "# For actions, this is the corresponding dictionary:\n",
    "action_names = {0:'L', 1:'D', 2:\"R\", 3:\"U\"}\n",
    "\n",
    "\"\"\"\n",
    "Here, \n",
    "'L' means left\n",
    "'D' means down\n",
    "'R' means right\n",
    "'U' means up\n",
    "\n",
    "\n",
    "\n",
    "You can access these tuples by simply env.P[s][a].\n",
    "where 's' is state, and 'a' is action. For example, let's say we are at state '4',\n",
    "and we take an action '1' or \"Down\". The next state (ns) would be 8, the episode would not have ended (is_done), \n",
    "the reward (r) is 0 and the transition probabilty (prob) is 1 because this is a deterministic setting.\n",
    "\"\"\"\n",
    "\n",
    "prob, ns, r, is_done = env.P[4][1][0]\n",
    "\n",
    "print(\"Transition Probabilty: \", prob)\n",
    "print(\"Next State: \", ns)\n",
    "print(\"Reward: \", r)\n",
    "print(\"Episode ended? : \", is_done)\n",
    "# Note that we need to add a [0] after env.P[s][a] because it returns a list containing the tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration \n",
    "\n",
    "- Follow the pseudo-code given in the handout for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "action_names = {0:'L', 1:'D', 2:\"R\", 3:\"U\"}\n",
    "print(action_names[1])\n",
    "# policy = np.random.randint(0, 4, size=env.nS)\n",
    "# print(enumerate(policy))\n",
    "\n",
    "\n",
    "# # pi = np.zeros(env.nS, dtype=int)\n",
    "# # print(pi)\n",
    "# print(pi.reshape([4,-1]))\n",
    "# pi = np.array([1,2,1,0,1,0,1,0,2,1,1,0,0,2,2,0])\n",
    "# print(pi.reshape([4,-1]))\n",
    "# states = 16\n",
    "# policy_print = np.array([action_names[x] for x in pi])\n",
    "# print(np.array(policy_print).reshape([-1,4]))\n",
    "\n",
    "def print_policy(policy, action_names, states):\n",
    "    \"\"\"Print the policy in human-readable format.\n",
    "    If you've implemented this correctly, the output (for 4x4 map) should be:\n",
    "    \n",
    "    D R D L \n",
    "    D L D L \n",
    "    R D D L \n",
    "    L R R L \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.ndarray\n",
    "        Array of state to action number mappings\n",
    "    action_names: dict\n",
    "        Mapping of action numbers to characters representing the action.\n",
    "    num_states: int\n",
    "        Number of states in the FrozenLakeEnvironment (16 or 64 for 4x4 or 8x8 maps respectively)      \n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.matshow(policy.reshape([-1,4]), cmap='YlGnBu')\n",
    "    for (i,j), z in np.ndenumerate(policy.reshape([-1,4])):\n",
    "        ax.text(j, i, action_names[z], ha='center',va='center')\n",
    "    plt.show()\n",
    "#     pi_read = np.array([action_names[x] for x in policy])\n",
    "#     print(np.array(pi_read).reshape([-1,4]))\n",
    "#     pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(env, state, value_func, gamma):\n",
    "    \"\"\"\n",
    "    Helps calculate the value for all action in a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: Frozen Lake Environment\n",
    "      The environment to comput value iteration for.\n",
    "    state: int\n",
    "      The state to consider.\n",
    "    value_func: np.array\n",
    "      The value function to use as an estimator.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A: np.array\n",
    "      A vector of length env.nA containing the expected value of \n",
    "      each iteration\n",
    "    \"\"\"\n",
    "#     A = np.zeros(env.nA)\n",
    "#     val = 0\n",
    "#     for a in range(env.nA):\n",
    "#         for prob, next_state, reward, done in env.model[state][a]:\n",
    "#             val += prob *(reward + gamma*value_func[next_state])\n",
    "#         A[a] = val\n",
    "    action_values = []\n",
    "    state_value = 0\n",
    "    for action in range(env.nA):\n",
    "        for i in range(len(env.P[state][action])):\n",
    "            prob, next_state, reward, done = env.P[state][action][i]\n",
    "            state_action_value = prob * reward + (gamma*value_func[next_state])\n",
    "            state_value += state_action_value\n",
    "        action_values.append(state_value)\n",
    "            \n",
    "    return np.asarray(action_values)\n",
    "\n",
    "def q_value(env, s, value_func, gamma):\n",
    "    q = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q[a] += prob*(reward + gamma * value_func[next_state])\n",
    "    return q\n",
    "\n",
    "# Q = np.zeros([env.nA, env.sA])\n",
    "# for s in range(env.nS):\n",
    "#     Q[s] = q_value(env, V, s)\n",
    "# print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "[1.       1.99     2.9701   6.880798 1.       1.       2.98     1.\n",
      " 1.       2.98     1.       1.       1.       1.99     2.9701   1.      ]\n",
      "16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_policy_sync(env, gamma, policy, value_func, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "    \n",
    "    Evaluates the value of a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: Frozen Lake Environment\n",
    "      The environment to compute value iteration for.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    value_func: np.array\n",
    "      Array of scalar values for each state\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "    val_iter=0\n",
    "    new_value_func = np.zeros(env.nS)\n",
    "    for val_iter in range(max_iterations): \n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            Vs = 1\n",
    "            for a, action_prob in enumerate([policy[s]]):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    if next_state == s:\n",
    "                        Vs += reward\n",
    "                    else:\n",
    "                        Vs += action_prob * prob * (reward + gamma * value_func[next_state])\n",
    "            delta = max(delta, np.abs(value_func[s] - Vs))\n",
    "            value_func[s] = Vs\n",
    "        if delta < tol:\n",
    "            break\n",
    "    return value_func, val_iter\n",
    "    \n",
    "#     val_iter=0\n",
    "#     new_value_func = np.zeros(env.nS)\n",
    "#     for val_iter in range(max_iterations): \n",
    "#         delta = 0\n",
    "#         # For each state, perform a \"full backup\"\n",
    "#         for s in range(env.nS):\n",
    "#             old_value = value_func\n",
    "#             a = policy[s]\n",
    "#             for prob, next_state, reward, done in env.P[s][a]:\n",
    "#                 if next_state == s:\n",
    "#                     new_value_func[s] = reward\n",
    "#                 else:\n",
    "#                     new_value_func[s] = action_prob * prob * (reward + gamma * value_func[next_state])\n",
    "#             delta = np.abs(old_value - new_value_func)\n",
    "#         if np.all(delta < tol):\n",
    "#             break\n",
    "#     return new_value_func, val_iter\n",
    "\n",
    "env = FrozenLakeEnv(map_name =\"4x4\", is_slippery=False)\n",
    "env.reset()\n",
    "action = 1\n",
    "(observation, reward, done, prob) = env.step(action)\n",
    "env.render()\n",
    "action = 1\n",
    "(observation, reward, done, prob) = env.step(action)\n",
    "env.render()\n",
    "action = 1\n",
    "(observation, reward, done, prob) = env.step(action)\n",
    "env.render()\n",
    "\n",
    "\n",
    "# Checking #\n",
    "policy = np.random.randint(0, 4, size=env.nS)\n",
    "value_func = np.zeros(env.nS)\n",
    "gamma = 0.99\n",
    "V, val_iter = evaluate_policy_sync(env, gamma, policy, value_func)\n",
    "print(V)\n",
    "import seaborn as sns\n",
    "plt.figure()\n",
    "sns.heatmap(V.reshape(4,4), cmap='YlGnBu', annot=True, cbar=False)\n",
    "S_n = env.observation_space\n",
    "A_n = env.action_space\n",
    "print(S_n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(env, gamma, value_func, policy):\n",
    "    \"\"\"Performs policy improvement.\n",
    "    \n",
    "    Given a policy and value function, improves the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: Frozen Lake Environment\n",
    "      The environment to compute value iteration for.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    value_func: np.ndarray\n",
    "      Value function for the given policy.\n",
    "    policy: dict or np.array\n",
    "      The policy to improve. Maps states to actions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool, np.ndarray\n",
    "      Returns the new imporved policy.\n",
    "    \"\"\"\n",
    "    for s in range(env.nS):\n",
    "        old_action = policy[s]\n",
    "        action_values = q_value(env, s, value_func, gamma)\n",
    "        policy[s] = np.argmax(action_values)\n",
    "        \n",
    "    return policy\n",
    "\n",
    "#     return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def policy_iteration_sync(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    See page 85 of the Sutton & Barto Second Edition book.\n",
    "\n",
    "    You should call the improve_policy() and evaluate_policy_sync() methods to\n",
    "    implement this method.\n",
    "    \n",
    "    If you've implemented this correctly, it should take much less than 1 second.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env: Frozen Lake Environment\n",
    "      The environment to compute value iteration for.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    policy = np.random.randint(0, 4, size=env.nS)   #Define random policy\n",
    "    value_func = np.zeros(env.nS)    # Define initial value function\n",
    "    num_pol_iter = 0\n",
    "    num_val_iter = 0\n",
    "    value_iter_list = []\n",
    "    for i in range(max_iterations):\n",
    "        value_func, val_iter = evaluate_policy_sync(env, gamma, policy, value_func)\n",
    "        num_val_iter += val_iter\n",
    "        value_iter_list.append(value_func)\n",
    "        new_policy = improve_policy(env, gamma, value_func, policy)\n",
    "        num_pol_iter +=1\n",
    "        policy=new_policy.copy()\n",
    "        delta = policy-new_policy\n",
    "        if delta.all() < tol:\n",
    "            break\n",
    "        \n",
    "        \n",
    "\n",
    "    return policy, value_func, num_pol_iter, num_val_iter, value_iter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time for value iterations is  0.0008482933044433594  seconds\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.        1.99      6.9103   21.523591]\n",
      " [ 1.        1.        3.97      1.      ]\n",
      " [ 1.        1.99      6.9103    1.      ]\n",
      " [ 1.        2.98      1.        1.      ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from plot_utils import plot_values\n",
    "def main():\n",
    "    \n",
    "    # WRITE YOUR CODE HERE:\n",
    "    env = FrozenLakeEnv(map_name =\"4x4\", is_slippery=False)\n",
    "    gamma = 0.99\n",
    "    startTime = time.time()\n",
    "    policy, value_func, num_pol_iter, num_val_iter, value_iter_list = policy_iteration_sync(env, gamma)\n",
    "    \n",
    "    endTime = time.time()\n",
    "    print(\"The running time for value iterations is \", endTime-startTime, ' seconds')\n",
    "    print(num_pol_iter)\n",
    "    print(num_val_iter)\n",
    "    action_names = {0:'L', 1:'D', 2:\"R\", 3:\"U\"}\n",
    "    print_policy(policy, action_names, env.nS)\n",
    "    print(value_func.reshape([-1,4]))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title('Value Function of the policy')\n",
    "    ax.matshow(value_func.reshape([-1,4]), cmap='YlGnBu')\n",
    "    \n",
    "#     plot_values(value_func)\n",
    "    \n",
    "#     env.render()\n",
    "#     obs, reward, done, _ = env.step(policy[env.s])\n",
    "#     while done is False:\n",
    "#         env.render()\n",
    "#         obs, reward, done, _, env.step(policy[obs])\n",
    "#     env.render()\n",
    "# Plot a graph showing the value function of all states as a function of number of policy iterations\n",
    "# for 4 by 4 maps.  \n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
